{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"text.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMqz56XlsjU2rSyfbMf0cm3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"source":["!pip install transformers"],"outputs":[],"metadata":{"id":"IqC9g0EanGg5"}},{"cell_type":"code","execution_count":5,"source":["import tensorflow as tf\r\n","import numpy as np\r\n","from tensorflow.keras.models import Model\r\n","from tensorflow.keras.optimizers import Adam\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split\r\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n","from transformers import BertTokenizer, TFBertModel, TFTrainer, TFTrainingArguments\r\n","from pandas import DataFrame\r\n","\r\n","# データを読み込む\r\n","data = pd.read_csv('/content/treated.csv')\r\n","\r\n","# 訓練用データを取得\r\n","train = data.dropna(subset=['target'])\r\n","train_texts, train_labels = train['text'], train['target']\r\n","\r\n","# テスト用データを取得\r\n","test = data[data['target'].isnull()]\r\n","test_texts = test['text']\r\n","\r\n","# BERT用のエンコーダーを読み込む\r\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\r\n","\r\n","# テキストを BERT 用にエンコーディング\r\n","train_encodings = tokenizer(train_texts.to_list(),\r\n","                            padding='max_length',\r\n","                            truncation=False,\r\n","                            return_attention_mask=True,\r\n","                            max_length=100)\r\n","\r\n","test_encoding = tokenizer(test_texts.to_list(),\r\n","                          padding='max_length',\r\n","                          truncation=False,\r\n","                          return_attention_mask=True,\r\n","                          max_length=100)\r\n","\r\n","\r\n","def encode_tf_layers(encoding):\r\n","    dict_encoding = dict(encoding)\r\n","    input_id = np.asarray(dict_encoding['input_ids'])\r\n","    attention_id = np.asarray(dict_encoding['attention_mask'])\r\n","    return input_id, attention_id\r\n","\r\n","\r\n","# エンコードしたデータから単語IDとアテンションを取り出す\r\n","train_ids, train_att = encode_tf_layers(train_encodings)\r\n","test_ids, test_att = encode_tf_layers(test_encoding)\r\n","\r\n","# BERT のモデルを読み込む\r\n","bert_model = TFBertModel.from_pretrained('bert-large-uncased')\r\n","\r\n","# モデル構築\r\n","input = tf.keras.Input(shape=(100,), dtype='int32')\r\n","attention_masks = tf.keras.Input(shape=(100,), dtype='int32')\r\n","output = bert_model([input, attention_masks])\r\n","output = output[1]\r\n","output = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal')(output)\r\n","output = tf.keras.layers.Dropout(0.2)(output)\r\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\r\n","model = tf.keras.models.Model(\r\n","    inputs=[input, attention_masks], outputs=output)\r\n","\r\n","model.summary()\r\n","\r\n","model.compile(\r\n","    loss='binary_crossentropy',\r\n","    optimizer=Adam(learning_rate=6e-6),\r\n","    metrics=['accuracy']\r\n",")\r\n","\r\n","history = model.fit(\r\n","    [train_ids, train_att], train_labels,\r\n","    epochs=2,\r\n","    batch_size=4,\r\n","    validation_split=0.2,\r\n",")\r\n","\r\n","# 推論\r\n","predict = model.predict([test_ids, test_att])\r\n","predict = (predict > 0.5).astype(int).reshape(-1)\r\n","\r\n","# CSV に出力\r\n","sample_data = pd.read_csv('/content/sample_submission.csv')\r\n","submit_data = DataFrame(\r\n","    {'id': sample_data['id'].to_list(), 'target': predict.tolist()})\r\n","submit_data.to_csv('/content/result.csv', index=False)\r\n"],"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 100)]        0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 100)]        0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 335141888   input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 32)           32800       tf_bert_model_1[0][1]            \n","__________________________________________________________________________________________________\n","dropout_147 (Dropout)           (None, 32)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            33          dropout_147[0][0]                \n","==================================================================================================\n","Total params: 335,174,721\n","Trainable params: 335,174,721\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/2\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","1523/1523 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.7251WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","1523/1523 [==============================] - 585s 367ms/step - loss: 0.5576 - accuracy: 0.7251 - val_loss: 0.4563 - val_accuracy: 0.7971\n","Epoch 2/2\n","1523/1523 [==============================] - 555s 364ms/step - loss: 0.4170 - accuracy: 0.8258 - val_loss: 0.4141 - val_accuracy: 0.8378\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]}],"metadata":{"id":"Hn7s-Cg7mSH-","cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627649432154,"user_tz":-540,"elapsed":1288459,"user":{"displayName":"小林春斗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj14jYjAXK6tDwH4Ol8QmOn4rVVttr8Xq3EbF_j=s64","userId":"09506590058739955272"}},"outputId":"a606dd61-9379-470f-d43c-65ef672aafd9"}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\r\n","import numpy as np\r\n","from tensorflow.keras.models import Model\r\n","from tensorflow.keras.optimizers import Adam\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split\r\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n","from transformers import RobertaTokenizer, TFRobertaModel, TFTrainer, TFTrainingArguments\r\n","from pandas import DataFrame\r\n","\r\n","# データを読み込む\r\n","data = pd.read_csv('/content/treated.csv')\r\n","\r\n","# 訓練用データを取得\r\n","train = data.dropna(subset=['target'])\r\n","train_texts, train_labels = train['text'], train['target']\r\n","\r\n","# テスト用データを取得\r\n","test = data[data['target'].isnull()]\r\n","test_texts = test['text']\r\n","\r\n","# BERT用のエンコーダーを読み込む\r\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\n","\r\n","# テキストを BERT 用にエンコーディング\r\n","train_encodings = tokenizer(train_texts.to_list(),\r\n","                            padding='max_length',\r\n","                            truncation=False,\r\n","                            return_attention_mask=True,\r\n","                            max_length=100)\r\n","\r\n","test_encoding = tokenizer(test_texts.to_list(),\r\n","                          padding='max_length',\r\n","                          truncation=False,\r\n","                          return_attention_mask=True,\r\n","                          max_length=100)\r\n","\r\n","\r\n","def encode_tf_layers(encoding):\r\n","    dict_encoding = dict(encoding)\r\n","    input_id = np.asarray(dict_encoding['input_ids'])\r\n","    attention_id = np.asarray(dict_encoding['attention_mask'])\r\n","    return input_id, attention_id\r\n","\r\n","\r\n","# エンコードしたデータから単語IDとアテンションを取り出す\r\n","train_ids, train_att = encode_tf_layers(train_encodings)\r\n","test_ids, test_att = encode_tf_layers(test_encoding)\r\n","\r\n","# BERT のモデルを読み込む\r\n","bert_model = TFRobertaModel.from_pretrained('roberta-large')\r\n","\r\n","# モデル構築\r\n","input = tf.keras.Input(shape=(100,), dtype='int32')\r\n","attention_masks = tf.keras.Input(shape=(100,), dtype='int32')\r\n","output = bert_model([input, attention_masks])\r\n","output = output[1]\r\n","output = tf.keras.layers.Dense(\r\n","    32, activation='relu', kernel_initializer='he_normal')(output)\r\n","output = tf.keras.layers.Dropout(0.2)(output)\r\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\r\n","model = tf.keras.models.Model(\r\n","    inputs=[input, attention_masks], outputs=output)\r\n","\r\n","model.summary()\r\n","\r\n","model.compile(\r\n","    loss='binary_crossentropy',\r\n","    optimizer=Adam(learning_rate=6e-6),\r\n","    metrics=['accuracy']\r\n",")\r\n","\r\n","history = model.fit(\r\n","    [train_ids, train_att], train_labels,\r\n","    epochs=2,\r\n","    batch_size=4,\r\n","    validation_split=0.2,\r\n",")\r\n","\r\n","# 推論\r\n","predict = model.predict([test_ids, test_att])\r\n","predict = (predict > 0.5).astype(int).reshape(-1)\r\n","\r\n","# CSV に出力\r\n","sample_data = pd.read_csv('/content/sample_submission.csv')\r\n","submit_data = DataFrame(\r\n","    {'id': sample_data['id'].to_list(), 'target': predict.tolist()})\r\n","submit_data.to_csv('/content/result.csv', index=False)\r\n"],"outputs":[],"metadata":{}}]}