{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMqz56XlsjU2rSyfbMf0cm3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IqC9g0EanGg5"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hn7s-Cg7mSH-","cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627649432154,"user_tz":-540,"elapsed":1288459,"user":{"displayName":"小林春斗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj14jYjAXK6tDwH4Ol8QmOn4rVVttr8Xq3EbF_j=s64","userId":"09506590058739955272"}},"outputId":"a606dd61-9379-470f-d43c-65ef672aafd9"},"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from transformers import BertTokenizer, TFBertModel, TFTrainer, TFTrainingArguments\n","from pandas import DataFrame\n","\n","# データを読み込む\n","data = pd.read_csv('/content/treated.csv')\n","\n","# 訓練用データを取得\n","train = data.dropna(subset=['target'])\n","train_texts, train_labels = train['text'], train['target']\n","\n","# テスト用データを取得\n","test = data[data['target'].isnull()]\n","test_texts = test['text']\n","\n","# BERT用のエンコーダーを読み込む\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","\n","# テキストを BERT 用にエンコーディング\n","train_encodings = tokenizer(train_texts.to_list(),\n","                            padding='max_length',\n","                            truncation=False,\n","                            return_attention_mask=True,\n","                            max_length=100)\n","\n","test_encoding = tokenizer(test_texts.to_list(),\n","                          padding='max_length',\n","                          truncation=False,\n","                          return_attention_mask=True,\n","                          max_length=100)\n","\n","\n","def encode_tf_layers(encoding):\n","    dict_encoding = dict(encoding)\n","    input_id = np.asarray(dict_encoding['input_ids'])\n","    attention_id = np.asarray(dict_encoding['attention_mask'])\n","    return input_id, attention_id\n","\n","\n","# エンコードしたデータから単語IDとアテンションを取り出す\n","train_ids, train_att = encode_tf_layers(train_encodings)\n","test_ids, test_att = encode_tf_layers(test_encoding)\n","\n","# BERT のモデルを読み込む\n","bert_model = TFBertModel.from_pretrained('bert-large-uncased')\n","\n","# モデル構築\n","input = tf.keras.Input(shape=(100,), dtype='int32')\n","attention_masks = tf.keras.Input(shape=(100,), dtype='int32')\n","output = bert_model([input, attention_masks])\n","output = output[1]\n","output = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal')(output)\n","output = tf.keras.layers.Dropout(0.2)(output)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n","model = tf.keras.models.Model(\n","    inputs=[input, attention_masks], outputs=output)\n","\n","model.summary()\n","\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=Adam(learning_rate=6e-6),\n","    metrics=['accuracy']\n",")\n","\n","history = model.fit(\n","    [train_ids, train_att], train_labels,\n","    epochs=2,\n","    batch_size=4,\n","    validation_split=0.2,\n",")\n","\n","# 推論\n","predict = model.predict([test_ids, test_att])\n","predict = (predict > 0.5).astype(int).reshape(-1)\n","\n","# CSV に出力\n","sample_data = pd.read_csv('/content/sample_submission.csv')\n","submit_data = DataFrame(\n","    {'id': sample_data['id'].to_list(), 'target': predict.tolist()})\n","submit_data.to_csv('/content/result.csv', index=False)\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 100)]        0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 100)]        0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 335141888   input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 32)           32800       tf_bert_model_1[0][1]            \n","__________________________________________________________________________________________________\n","dropout_147 (Dropout)           (None, 32)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            33          dropout_147[0][0]                \n","==================================================================================================\n","Total params: 335,174,721\n","Trainable params: 335,174,721\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/2\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","1523/1523 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.7251WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","1523/1523 [==============================] - 585s 367ms/step - loss: 0.5576 - accuracy: 0.7251 - val_loss: 0.4563 - val_accuracy: 0.7971\n","Epoch 2/2\n","1523/1523 [==============================] - 555s 364ms/step - loss: 0.4170 - accuracy: 0.8258 - val_loss: 0.4141 - val_accuracy: 0.8378\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"}]}]}